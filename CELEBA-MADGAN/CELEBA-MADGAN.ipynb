{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE DIR Already Exists\n",
      "DEVICE IS cuda:0\n",
      "Iters: 0 Starting Epoch - 0/60. See log.txt for more details\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/ipykernel_launcher.py:203: UserWarning: /home/test/Desktop/NEW/CELEBA-MADGAN/CELEBA-is_degan1&epc=60sharingFalselrd=1e-05&lrg=0.0001&noise=1 DIRECTORY ALREADY EXISTS. YOU ARE OVERWRITING EXISTING DATA\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 4 and 8 in dimension 1 at /opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/generic/THCTensorMath.cu:71",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5f2375e08810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mD_Fake_Output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_out_d_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_b_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mD_Output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD_output_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_Fake_Output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mNOISE_INTERVAL\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 4 and 8 in dimension 1 at /opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/generic/THCTensorMath.cu:71"
     ]
    }
   ],
   "source": [
    "import argparse, sys\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch.distributions as dist\n",
    "import warnings\n",
    "\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "\n",
    "from CelebASharedGenerator import CelebASharedGenerator\n",
    "from CelebAUnsharedGenerator import CelebAUnsharedGenerator\n",
    "from ResidualDiscriminator import ResidualDiscriminator\n",
    "\n",
    "\n",
    "import utils\n",
    "from Logger import Logger\n",
    "import Losses\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from celeba_madgan_params import ARGS #import the paramters file\n",
    "\n",
    "\n",
    "\n",
    "#add the command line arguments\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--epochs', help='Number of epochs to run. Default=80', default=80, type =int)\n",
    "# parser.add_argument('--gpu', help='Use 0 for CPU and 1 for GPU. Default=1', default=1, type =int)\n",
    "# parser.add_argument('--noise', help='Use 0 for CPU and 1 for GPU. Default=1', default=1, type =int)\n",
    "# parser.add_argument('--num_channels', help='Number of channels in the real images in the real image dataset. Default=3', default=3, type=int)\n",
    "# parser.add_argument('--image_size', help='The size to which the input images will be resized. Default=64', default=64, type=int)\n",
    "# parser.add_argument('--leaky_slope', help='The negative slope of the Leaky ReLU activation used in the architecture. Default=0.2', default=0.2, type=float)\n",
    "# parser.add_argument('--dataroot', help='The parent dir of the dir(s) that contain the data. Default=\\'./data\\'', default='./data', type =str),\n",
    "# parser.add_argument('--n_z', help='The size of the noise vector to be fed to the generator. Default=100', default=100, type=int)\n",
    "# parser.add_argument('--batch_size', help='The batch size to be used while training. Default=120', default=120, type=int)\n",
    "# parser.add_argument('--num_generators', help='Number of generators to use. Default=3', default=3, type=int)\n",
    "# parser.add_argument('--degan', help ='1 if want to use modified loss function otherwise 0. Default=0', default=0, type=int)\n",
    "# parser.add_argument('--sharing', help='1 if you want to use the shared generator. 0 otherwise. Default=0', default=0, type=int)\n",
    "# parser.add_argument('--gpu_add', help='Address of the GPU you want to use. Default=0', default=0, type=int)\n",
    "# parser.add_argument('--lrg', help='Learning rate for the generator', default=1e-4, type=float)\n",
    "# parser.add_argument('--lrd', help='Learning rate for the discriminator', default=1e-5, type=float)\n",
    "# parser.add_argument('--bt1', help='Beta 1 parameter of the Adam Optimizer. Default=0.5', default=0.5, type=float)\n",
    "# parser.add_argument('--bt2', help='Beta 2 parameter of the Adam Optimizer. Default=0.999', default=0.999, type=float)\n",
    "# parser.add_argument('--ni', help='Noise degaradation interval. Default=1000', default=1000, type=int)\n",
    "# parser.add_argument('--ndf', help='Noise degradation factor. Default=0.98', default=0.98, type=float)\n",
    "# parser.add_argument('--nd', help='Noise standard dev. Default=0.1', default=0.1, type=float)\n",
    "# parser.add_argument('--nm', help='Noise mean. Default=0.0', default=0.0, type=float)\n",
    "# parser.add_argument('--chk_interval', help='Check Interval. Default=500', default=500, type=int)\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "num_epochs = 60\n",
    "is_gpu = 1\n",
    "add_noise = 1\n",
    "num_channels = 3\n",
    "image_size = 64\n",
    "leaky_slope = 0.2\n",
    "dataroot = './data'\n",
    "n_z = 100\n",
    "batch_size = 120\n",
    "num_generators = 3\n",
    "is_degan = 1\n",
    "sharing = 0\n",
    "gpu_add = 0\n",
    "lrd = 1e-5\n",
    "lrg = 1e-4\n",
    "beta1 =0.5\n",
    "beta2 = 0.999\n",
    "NOISE_INTERVAL=1000\n",
    "NOISE_DEGRADATION_FACTOR=0.98\n",
    "NOISE_DEV=0.1\n",
    "NOISE_MEAN = 0.0\n",
    "CHECK_INTERVAL = 500\n",
    "\n",
    "if sharing==1:\n",
    "    is_sharing=True\n",
    "else:\n",
    "    is_sharing=False\n",
    "\n",
    "CWD = os.getcwd()\n",
    "\n",
    "SUB_DIR = 'CELEBA-is_degan'+str(is_degan)+'&epc='+str(num_epochs)+'sharing'+str(is_sharing)+'lrd='+str(lrd)+'&lrg='+str(lrg)+'&noise='+str(add_noise)\n",
    "SAVE_DIR = str(CWD)+'/'+SUB_DIR\n",
    "\n",
    "try:\n",
    "    os.mkdir(SAVE_DIR)\n",
    "except:\n",
    "    print(\"SAVE DIR Already Exists\")\n",
    "\n",
    "#Init the Logger defined in Logger.py\n",
    "logger = Logger(SAVE_DIR+'/log.txt')\n",
    "\n",
    "device = torch.device(\"cuda:\"+str(gpu_add) if (torch.cuda.is_available() and is_gpu > 0) else \"cpu\")\n",
    "print(\"DEVICE IS {}\".format(device))\n",
    "################################\n",
    "\n",
    "\"\"\"\n",
    "This section is for raising warnings/exceptions related to the command line args\n",
    "\"\"\"\n",
    "############################################################################################\n",
    "if(is_gpu>1 or is_gpu<0):\n",
    "    raise ValueError(\"gpu arg is either one or zero. You entered {}\".format(is_gpu))\n",
    "if(num_channels<=0):\n",
    "    raise ValueError(\"num_channels has to be greater than 0. You entered {}\".format(num_channels))\n",
    "if(image_size<=0):\n",
    "    raise ValueError(\"image_size has to be greater than 0. You entered {}\".format(image_size))\n",
    "if(leaky_slope>0.5):\n",
    "    warnings.warn(\"the negative slope argument of the LeakyReLU activation is unusually low. You entered {}\".format(leaky_slope))\n",
    "if(os.path.isdir(dataroot)==False):\n",
    "    raise FileNotFoundError(\"the path specified in dataroot is not valid. You entered {}\".format(dataroot))\n",
    "if(n_z<64):\n",
    "    warnings.warn(\"The length of the noise vector is unusually low. You entered {}\".format(n_z))\n",
    "if(batch_size<=0):\n",
    "    raise ValueError(\"Invalid batch size. Has to be greater than zero. You entered {}\".format(batch_size))\n",
    "if(num_generators<=0):\n",
    "    raise ValueError(\"Invalid number of generators. Has to be greater than zero. You entered {}\".format(num_generators))\n",
    "if(is_degan<0 or is_degan>1):\n",
    "    raise ValueError(\"degan parameter is either zero or one. You entered {}\".format(is_degan))\n",
    "if(is_sharing<0 or is_sharing>1):\n",
    "    raise ValueError(\"sharing parameter is either zero or one. You entered {}\".format(is_sharing))\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This section deals with loading the data\n",
    "\"\"\"\n",
    "################################\n",
    "#Function which returns the dataloader\n",
    "def get_dataloader():\n",
    "    dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "    return dataloader\n",
    "################################\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Initialize the weights in this cell\n",
    "\"\"\"\n",
    "################################\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, ARGS['conv_weights_init_mean'], ARGS['conv_weights_init_dev'])\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, ARGS['bn_weights_init_mean'], ARGS['bn_weights_init_dev'])\n",
    "        nn.init.constant_(m.bias.data, ARGS['bn_bias_weights_init'])\n",
    "################################\n",
    "\n",
    "\"\"\"\n",
    "Initialize the generator and the discriminator and dataloader\n",
    "\"\"\"\n",
    "##########################################\n",
    "dataloader = get_dataloader()\n",
    "\n",
    "if is_sharing==False:\n",
    "    # generator = MNISTUnsharedGenerator(num_generators, n_z,  batch_size).to(device)\n",
    "    generator = CelebAUnsharedGenerator(n_z, num_channels).to(device)\n",
    "else:\n",
    "    generator = CelebASharedGenerator(n_z, num_channels).to(device)\n",
    "\n",
    "generator.apply(weights_init)\n",
    "\n",
    "discriminator = ResidualDiscriminator(num_channels, leaky_slope, num_generators).to(device)\n",
    "discriminator.apply(weights_init)\n",
    "##########################################\n",
    "\n",
    "\"\"\"\n",
    "Init the optimizers for the generator and the discriminator and the losses\n",
    "\"\"\"\n",
    "##########\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimD = torch.optim.Adam(discriminator.parameters(), lr = lrd, betas = (beta1, beta2))\n",
    "optimG = torch.optim.Adam(generator.parameters(), lr = lrg,  betas = (beta1, beta2))\n",
    "##########\n",
    "\n",
    "\"\"\"\n",
    "Create the directories for storing the results\n",
    "\"\"\"\n",
    "######################\n",
    "if os.path.isdir(SAVE_DIR):\n",
    "    warnings.warn(\"{} DIRECTORY ALREADY EXISTS. YOU ARE OVERWRITING EXISTING DATA\".format(SAVE_DIR))\n",
    "else:\n",
    "    os.mkdir(SAVE_DIR)\n",
    "\n",
    "if not os.path.isdir(SAVE_DIR+'/Results'):\n",
    "        os.mkdir(SAVE_DIR+'/Results')\n",
    "\n",
    "######################\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Init the loss lists for G and D\n",
    "\"\"\"\n",
    "#########\n",
    "D_losses = []\n",
    "G_losses = []\n",
    "#########\n",
    "\n",
    "iters=0\n",
    "\n",
    "num_batches = len(dataloader)\n",
    "\n",
    "DEBUG=True\n",
    "\n",
    "fixed_noise = utils.generate_noise_for_generator(batch_size//num_generators, n_z, device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Iters: {} Starting Epoch - {}/{}. See log.txt for more details\".format(iters, epoch, num_epochs))\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # print(\"hello\")\n",
    "        ############################################\n",
    "        #Train the discriminator first\n",
    "        ############################################\n",
    "    \n",
    "        discriminator.zero_grad()\n",
    "        #1. Train D on real data\n",
    "        #fetch natch of real images\n",
    "        real_images_batch = data[0].to(device)\n",
    "        real_b_size = real_images_batch.size(0)\n",
    "\n",
    "        if real_b_size!=batch_size:\n",
    "            continue\n",
    "\n",
    "        #generate labels for the real batch of data...the (k+1)th element is 1...rest are zero\n",
    "        D_label_real  = utils.get_labels(num_generators, -1, real_b_size, device)\n",
    "\n",
    "        #forward pass for the real batch of data and then resize  \n",
    "        \n",
    "        gen_input_noise = utils.generate_noise_for_generator(real_b_size//num_generators, n_z, device)\n",
    "        gen_output = generator(gen_input_noise)#, real_b_size//num_generators)\n",
    "        \n",
    "        gen_out_d_in = gen_output.detach()\n",
    "        ##############################################################\n",
    "        norm = dist.Normal(torch.tensor([NOISE_MEAN]), torch.tensor([NOISE_DEV]))\n",
    "\n",
    "        if add_noise==1:\n",
    "            x_noise = norm.sample(gen_out_d_in.size()).view(gen_out_d_in.size()).to(device)\n",
    "            gen_out_d_in = gen_out_d_in + x_noise \n",
    "        \n",
    "        \n",
    "        #################################################################\n",
    "        D_Label_Fake =[]\n",
    "        for g in range(num_generators):\n",
    "            D_Label_Fake.append(utils.get_labels(num_generators, g, real_b_size//num_generators, device))\n",
    "        \n",
    "        D_Label_Fake = torch.cat(D_Label_Fake)\n",
    "        D_Labels = torch.cat([D_label_real, D_Label_Fake])\n",
    "\n",
    "        if DEBUG: logger.log(str(D_Labels))\n",
    "        \n",
    "        D_output_real = discriminator(real_images_batch).view((real_b_size,-1))\n",
    "        D_Fake_Output = discriminator(gen_out_d_in).view((real_b_size, -1))\n",
    "\n",
    "        D_Output = torch.cat([D_output_real, D_Fake_Output])\n",
    "        \n",
    "        if iters%NOISE_INTERVAL==0:\n",
    "            NOISE_DEV=NOISE_DEV*NOISE_DEGRADATION_FACTOR\n",
    "            logger.log(\"NOISE DEV IS NOW :{}\".format(NOISE_DEV))\n",
    "\n",
    "       \n",
    "        if is_degan==1:\n",
    "            err_D = Losses.D_Loss(D_Fake_Output, D_output_real, D_Label_Fake, loss, num_generators)\n",
    "        else:\n",
    "            err_D = loss(D_Output, D_Labels)\n",
    "\n",
    "\n",
    "        err_D.backward(retain_graph=True)\n",
    "\n",
    "        optimD.step()\n",
    "\n",
    "        ########################################\n",
    "        #Train the generators\n",
    "        ########################################\n",
    "\n",
    "        generator.zero_grad()\n",
    "\n",
    "        if add_noise==1:\n",
    "            D_Fake_Output_G = discriminator(gen_output+x_noise).view((real_b_size, -1))\n",
    "        else:\n",
    "            D_Fake_Output_G = discriminator(gen_output).view((real_b_size, -1))\n",
    "\n",
    "        G_Labels = utils.get_labels(num_generators, -1, D_Fake_Output_G.size(0),  device)\n",
    "\n",
    "        if is_degan==1:\n",
    "            err_G = Losses.G_Loss(D_Fake_Output_G, D_output_real, D_Label_Fake, loss, num_generators)\n",
    "        else:\n",
    "            err_G = loss(D_Fake_Output_G, G_Labels)\n",
    "\n",
    "\n",
    "        err_G.backward()\n",
    "\n",
    "        optimG.step()\n",
    "\n",
    "\n",
    "        if iters%CHECK_INTERVAL==0:\n",
    "            logger.log(\"Iters: {}; Epo: {}/{}; Btch: {}/{}; D_Err: {}; G_Err: {};\".format(iters, epoch, num_epochs, i,num_batches,  err_D.item(), err_G.item()))\n",
    "\n",
    "\n",
    "        #add to the dicts for keeping track of losses\n",
    "        D_losses.append(err_D.item())\n",
    "        G_losses.append(err_G.item())\n",
    "\n",
    "\n",
    "\n",
    "        if (iters % CHECK_INTERVAL == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixed_noise).detach().cpu()\n",
    "            obs_size = fake.size(0)\n",
    "            obs_size=obs_size//num_generators\n",
    "\n",
    "            for g in range(num_generators):\n",
    "                io.imsave(SAVE_DIR+'/Results/'+str(iters)+'_G'+str(g)+'.png', np.transpose(vutils.make_grid(fake[g*obs_size: (g+1)*obs_size], padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "        \n",
    "        iters = iters+1\n",
    "        DEBUG=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Save the model and the params to file\n",
    "\"\"\"\n",
    "para_dict = {\n",
    "    'args':args,\n",
    "    'g_state_dict':generator.state_dict(),\n",
    "    'optim_g_state_dict':optimG.state_dict(),\n",
    "    'd_state_dict':discriminator.state_dict(),\n",
    "    'optim_d_state_dict': optimD.state_dict(),\n",
    "    'd_losses':D_losses,\n",
    "    'g_losses':G_losses,\n",
    "}\n",
    "\n",
    "PTH_SAVE_PATH = SAVE_DIR+'/model_save.pth'\n",
    "\n",
    "utils.save_model(PTH_SAVE_PATH, para_dict)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "generate samples for FID comparison and save in folder\n",
    "\"\"\"\n",
    "output_batch_size = 120\n",
    "\n",
    "output_batch_size_factor=100\n",
    "\n",
    "\n",
    "for g in range(num_generators):\n",
    "    if not os.path.isdir(SAVE_DIR+'/Results/G'+str(g)):\n",
    "        os.mkdir(SAVE_DIR+'/Results/G'+str(g))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for o_b_s in range(output_batch_size_factor):\n",
    "        \n",
    "        fixed_noise = utils.generate_noise_for_generator(output_batch_size//num_generators, n_z, device)\n",
    "\n",
    "        fake = generator(fixed_noise).detach().cpu()\n",
    "        obs_size = fake.size(0)\n",
    "        obs_size=obs_size//num_generators\n",
    "        for g in range(num_generators):\n",
    "            for obs in range(obs_size):\n",
    "                io.imsave(SAVE_DIR+'/Results/G'+str(g)+'/'+str(o_b_s)+str(obs)+'.png', np.transpose(fake[g*obs_size+obs], (1,2,0)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
